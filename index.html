
<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags always come first -->
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title> NeurIPS 2023 Workshop: Self-Supervised Learning - Theory and Practice
</title>
  <link rel="canonical" href="./index.html">
  <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="./theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="./theme/css/style.css">
  <link rel="stylesheet" href="./theme/css/custom.css">


<meta name="description" content="NeurIPS 2023 Workshop Proposal Self-Supervised Learning: Theory and Practice">
</head>

<body>
  <header class="header">
    <div class="container">
      <div class="row">
        <!-- <div class="col-xs-2">
          <a href="./">
            <img class="img-fluid" src=./images/logo.png alt="ML4H: Machine Learning for Health">
          </a>
        </div> -->
        <div class="col-xs-10">
          <h1 class="title"><a href="../">NeurIPS 2023 Workshop: Self-Supervised Learning - Theory and Practice</a></h1>
          <br />
          <ul class="list-inline" style="font-weight:600"> 
            <li class="list-inline-item"><a href="../index.html">Home</a></li>
            <li class="list-inline-item"><a href="./pages/call-for-participation.html">Call for Papers</a></li>
            <li class="list-inline-item"><a href="./pages/schedule.html">Schedule</a></li>
            <li class="list-inline-item"><a href="./pages/speakers.html">Speakers</a></li>
            <li class="list-inline-item"><a href="./pages/organizers.html">Organizers</a></li>
            <li class="list-inline-item"><a href="./pages/program-committee.html">Program Committee</a></li>
            <li class="list-inline-item"><a href="./pages/accepted-paper.html">Accepted Papers</a></li>
          </ul>
        </div>
      </div>
    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>
</h1>
<article class="article">
  <div class="content">
    <div class="container">

<div class="row">
<!-- <div class="col-md-7"> -->
    <p>
   In-person workshop at <a href="https://nips.cc/Conferences/2023/Dates">NeurIPS 2023</a>
    </p>
    <p>
      <b>Date:</b> December 16, 2023
    </p>
    <p>
      <b>Contact Email</b>: sslneurips23@googlegroups.com
    </p>
    <p>
      <b>Room:</b> 217 -- 219
    </p>
    <p>
      <b>Time</b>: 9 AM -- 5:30 PM
    </p>
    <p>
      <b>Schedule</b>:
      <a href="./pages/schedule.html">here</a>
    </p>
  
  <!--p><b>For poster sessions: </b>posters can be viewed in the <a href='./pages/Accepted Paper.html'>Accepted Papers</a> page. If interested in a poster, you can click on the corresponding Zoom link. The authors will be presenting their posters in their Zoom.</p-->

  <p>
    Self-supervised learning (SSL) is an unsupervised approach for representation learning without relying on human-provided labels. It creates auxiliary tasks on unlabeled input data and learns representations by solving these tasks. SSL has demonstrated great success on images (e.g., MoCo, PIRL, SimCLR, DINO, MAE), speech (e.g., CPC, HuBERT, wav2vec) and text (e.g., word2vec, BERT, RoBERTa, GPT, OPT) and has shown promising results in other data modalities, including graphs, time-series, audio, etc. On a wide variety of tasks, without using human-provided labels, SSL achieves performance that is close to fully supervised approaches.
  </p>
  <p>
    The existing SSL research mostly focuses on improving the empirical performance without a theoretical foundation. While the proposed SSL approaches are empirically effective on benchmarks, they aren’t well understood from a theoretical perspective or practical use-cases. For example, why do certain auxiliary tasks in SSL perform better than others? How many unlabeled data examples are needed by SSL to learn a good representation? How is the performance of SSL affected by neural architectures? And practically, where do self-supervised models shine compared to traditional supervised models?
  </p>
    <!-- <div></div> -->
<!-- </div> -->


    <!-- </div> -->

<!-- <div class="text"> -->
    <p>In the 4th iteration of this workshop, we continue to bridge this gap between theory and practice. 
      We bring together SSL-interested researchers from various domains to discuss the theoretical foundations of empirically well-performing SSL approaches and how the theoretical insights can further improve SSL’s empirical performance.</p>
    <p>We invite submissions of both theoretical works and empirical works, and the intersection of the two. 
      The topics include but are not limited to: </p>
    <ul>
    <li>
    Theoretical foundations of SSL
    </li>
    <li>
    Sample complexity of SSL methods
    </li>
    <li>
    Theory-driven design of auxiliary tasks in SSL
    </li>
    <li>
    Comparative analysis of different auxiliary tasks
    </li>
    <li>
    Comparative analysis of SSL and supervised approaches
    </li>
    <li>
    Information theory and SSL
    </li>
    <li>
    SSL for computer vision, natural language processing, robotics, speech processing, time-series analysis, graph analytics, etc.
    </li>
    <li>
    SSL for healthcare, social media, neuroscience, biology, social science, etc.
    </li>
    <li>
    Cognitive foundations of SSL
    </li>
    </ul>
<!-- </div> -->
    <p>This year, this workshop will have a few invited talks by leading researchers from diverse backgrounds including theoretical ML, CV, NLP, robotics, etc. 
      The workshop will also have contributed talks, poster sessions, and panel discussion session to share perspectives on establishing foundational understanding of existing SSL approaches and theoretically-principled ways of developing new SSL methods. We accept short (max 4 pages) submissions. Each will be peer-reviewed by at least three reviewers. The accepted papers are allowed to be submitted to other conference venues. Please see our <a href="./pages/call-for-participation.html">call for papers </a> for details.
    </p>

    <p>
      Previous editions of our workshop: <a href="https://nips.cc/Conferences/2020/ScheduleMultitrack?event=16146">2020</a>, <a href="https://sslneuips21.github.io/">2021</a>, <a href="https://sslneuips22.github.io/">2022</a>.
    </p>

    
    <!-- <p>Apply for a <a href="https://forms.gle/SEc4Nj3RnmN6GN8e8">Travel Grant</a>.</p> -->



</div>


<!-- 
<div class="row">
<div class="alert alert-info" role="alert">
<p> <a href="./pages/schedule.html"> Day of Schedule </a>
<p> 2019/11/30 Schedule Posted </p>
</div>
</div>

<div class="row">
<div class="alert alert-info" role="alert">
<p>Poster Instructions</p>
<p> Size: 36 W X 48 H (inches) or 90 W X 122 H (cm) <br />
Paper Type: Light-weight (e.g., not-laminated) <br />
Attaching to Wall: There are no poster boards, we will have tape or some other means of attaching the posters to the wall.
<emph>
</emph>
</p>
</div>
</div> -->
<!-- <br>
<br>
<div class="row">
<div class="col-md-12">
<img src="./images/flagship.jpg" align="center">
<p> ML4H 2019 is proudly sponsored by Flagship Pioneering. Flagship Pioneering is a team of pioneering scientists and professionals who build companies that transform human health and sustainability. </p>
</div> -->
  </div>
</article>
    </div>
  </div>

  <!-- <footer class="footer">
    <div class="container">
      <div class="row">
       <ul class="col-sm-6 list-inline">


          <li class="list-inline-item"><a href="./categories.html">Categories</a></li>

        </ul>
        <p class="col-sm-6 text-sm-right text-muted">
          <a href="https://github.com/ml4health/ml4health.github.io">
          source on github
          </a>
          /
          <a href="https://github.com/getpelican/pelican" target="_blank">powered by Pelican</a>
          /
          <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
        </p>
      </div>
    </div>
  </footer> -->
</body>

</html>
